---
title: 'TV Show and Movie Popularity Analysis'
author:
  - Christian Garduno
  - Roshan Mehta
  - Colton Rowe
date: "6/3/23"
theme:
  dark: darkly
format:
  html:
    page-layout: full
    toc: true
    code-fold: true
editor: visual
jupyter: python3
---

::: panel-tabset
#### Home

## Abstract

The present article analyzes a dataset containing information about titles on different streaming services and build a model to predict the popularity of an arbitrary title. The datasets used were sourced from [Kaggle](https://www.kaggle.com/) and the specific links can be found on the `Data` tab above. These provide valuable insights into the popularity and characteristics of different titles available on the streaming platforms and will help us in predicting the outcomes of future titles.

## Problem Statement

The streaming services industry has seen rapid growth in recent years and has undoubtedly revolutionized the way we consume content. With numerous platforms offering a vast array of media types, it has become increasingly important for creators and streaming platforms to understand what makes a certain movie or TV show popular. This project aims to explore the Kaggle dataset and uncover the key factors that contribute to their popularity through fitting a series of different models.

## Outcomes
In an attempt to give media producers an idea of how popular their show or movie will be, we have developeed a web app...

## Findings
- Results of our work... (how good the models did, what the app can be used for, interpretation of EDA)

## Looking Forward
- Why is this work important?
  Media producers can use it for... 


- How could we have improved, what would we have wanted to add?

We have a column in our dataset `description` that describes each show and movie. I think it would have been interesting to use word vectoriztion and natural language processing (NLP) to quantify this. If we were to expand on this project, adding an NLP component would be one avenue to do so.
A neural network may have also been an interesting model to apply. Neural networks are considered to be highly flexible but not very interpretable, so adding this model to the app may or may not have been beneficial. In a practical sense, the model should be somewhat interpretable because that would best 

#### Data

## Source

We combined the below four datasets found on Kaggle to get a comprehensive dataset of movies and TV shows available on the major streaming platforms.

-   [Disney+](https://www.kaggle.com/datasets/shivamb/disney-movies-and-tv-shows)
-   [Hulu](https://www.kaggle.com/datasets/shivamb/hulu-movies-and-tv-shows)
-   [Amazon Prime](https://www.kaggle.com/datasets/shivamb/amazon-prime-movies-and-tv-shows)
-   [Netflix](https://www.kaggle.com/datasets/shivamb/netflix-shows)

*** Link where we got the API / which one it was ***
 

## Variable Description

The dataset contains both categorical and numeric variables which may provide insights for our analysis. Here is a brief description of the key variables:

-   **`type`**: Indicates whether the show is a TV series or a movie.
-   **`title`**: The title of the TV show.
-   **`director`**: The name of the director(s) of the show.
-   **`cast`**: The names of the main cast members.
-   **`country`**: The countries the show was released in.
-   **`release_year`**: The year when the show was released.
-   **`rating`**: The content rating assigned to the show (TV-14, PG-13, etc.).
-   **`duration`**: The number of seasons (for TV series) or the duration (minutes) of the movie (for movies).
-   **`listed_in`**: The genre(s) or category(s) the show belongs to.
-   **`description`**: A brief summary or description of the show.
-   **`score`**: Rating of the show or movie - scraped from [IMDb.](https://www.imdb.com/)
-   **`director_score`**: Calculated score based on the directors of the title.

Together, these variables provide a set of features that allow us to analyze and understand the characteristics that determine the popularity of TV shows. By exploring these variables and their relationships, we can gain insights into the factors that contribute to a show's popularity, like the impact of different genres or countries of origin, and begin to create better shows that more people would watch.

## Cleaning

### Original Data
- Original state of the data
- How good is the original data? How many patterns can we reasonably extract from it?

### API
- How we pulled the score using the API
- Talk about descrepencies between current and API score

### Director and cast data
- Talk about the chalenges of working with cast and director data (one hot encoding them would mean a lot of predictors)
- Add how you calculated average director and cast score, and why it is justified without data leakage (the splitting into two sets and only calculating on one thing)

### Turning Lists into Factors
- listed_in to genre
- Talk about why it was justified to reduce some of the ratings, ect (not rated -> NR)

## Overview

The response varaible that we used for prediction was the IMDB score of the Movie or TV Show. After combing the four datasets we found on Kaggle we discovered some of the shows had null values for their IMDB score. In order to replace these nulls we connected to an API that can be found [here](https://rapidapi.com/linaspurinis/api/mdblist). With this API we were able to query by Movie or Show Title and get it's IMDB score.

The next part was the cleaning and formating of our data so that it can be vectorized for our models. Since cast, director, and country of origin were all in lists, we had to split each list value into their own rows. Due to this spliting we can turn our categorical variables into numerical representation in order for our model to read it.

#### Exploratory Data Analysis (EDA)

In this section we will explore and visualize our dataset to gain a better understanding of what we are working with, identify any obvious patterns, correlations, or trends.

TODO: Add description / interpretation for each figure

```{python}
import pandas as pd
data = pd.read_csv("../Data/data/streaming_titles_final.csv")
```

## Figures

::: panel-tabset
```{python}
# | echo: False
import matplotlib.pyplot as plt

data = data
data.head()
```

```{python}
data.isna().sum()
```

We can verify that there are no missing values, as these were handled during the data cleaning step.

#### Size & Distribution

```{python}
print("We have " + str(len(data)) + " total different movies and TV shows that we are working with.")
```

```{python}
print(data.shape)
print("There are a total of 89 columns that were are working with.")
```

```{python}
import seaborn as sns

sns.countplot(x=data["type"])

# Add labels and title
plt.xlabel("Type")
plt.ylabel("Count")
plt.title("Distribution of Titles by Type")

# Display the plot
plt.show()
```

#### Line Chart

```{python}
# | fig.width: 10
# 1. Line Chart of the Number of Titles Released per Year

# Group data by year and count number of titles
counts = data.groupby("release_year")["title"].count()

# Create line chart
#plt.figure(figsize=(10,10))
plt.plot(counts.index, counts.values)

# Add labels and title
plt.xlabel("Year")
plt.ylabel("Number of Titles Released")
plt.title("Number of Titles Released per Year")

# Display chart
plt.show()
```

#### Content Ratings

```{python}
# | fig.width: 10

# 2. Histogram of the Distribution of Content Ratings

# Create histogram of content ratings
#plt.figure(figsize=(20,10))
plt.hist(data["rating"].dropna(), bins=10)

# Add labels and title
plt.xlabel("Content Rating")
plt.xticks(rotation=45)
plt.ylabel("Frequency")
plt.title("Distribution of Content Ratings")

# Display chart
plt.show()
```

#### Titles by Country

```{python}
# | fig.width: 10

# 3. Bar Chart of the Number of Titles per Country (count > 10)

# Extract first country from country column
chart = data.copy()
chart["country"] = chart["country"].str.split(", ").str[0]

# Group data by country and count number of titles
counts = chart.groupby("country")["title"].count()
counts = counts[counts > 10].sort_values(ascending=False)

# Create bar chart
#plt.figure(figsize=(20,10))
plt.bar(counts.index, counts.values)

# Add labels and title
plt.xlabel("Country")
plt.xticks(rotation=90)
plt.ylabel("Number of Titles")
plt.title("Number of Titles per Country")

# Display chart
plt.show()
```

#### IMDb Rating Vs. Runtime (Movies)

```{python}
# | fig.width: 10

# 4. Movie and Rating Scatter Plot

# Filter out TV shows and missing ratings
movies = data[(data["type"] == "Movie") & (data["score"].notnull())]

# Create scatter plot of IMDb rating vs. runtime
#plt.figure(figsize=(20,10))
plt.scatter(movies["score"], movies["duration"], alpha=0.5)

# Add labels and title
plt.xlabel("IMDb Rating")
plt.ylabel("Runtime (minutes)")
plt.title("IMDb Rating vs. Runtime for Movies")

# Display chart
plt.show()
```

#### IMDb Rating Vs. Runtime (TV Shows)

```{python}
# | fig.width: 10

# 5. TV Show and Rating Scatter Plot

# Filter out Movies and missing ratings
tv = data[(data["type"] == "TV Show") & (data["score"].notnull())]

# Create scatter plot of IMDb rating vs. runtime
#plt.figure(figsize=(20,10))
plt.scatter(tv["score"], tv["duration"], alpha=0.5)

# Add labels and title
plt.xlabel("IMDb Rating")
plt.ylabel("Runtime (seasons)")
plt.title("IMDb Rating vs. Runtime for TV Shows")

# Display chart
plt.show()
```

#### Top Genres

```{python}
# | fig.width: 10

# 6. Top 15 Genres By Number of Titles

# Get list of genre columns
genre_cols = [col for col in data.columns if col.startswith("genre")]

# Sum the number of true values in each genre column to get the total number of titles for each genre
genre_counts = data[genre_cols].sum()#.sort_values(ascending=False)

# Get the top 10 genres by number of titles
top_genres = genre_counts[:15]

# Remove "genre." from the genre names in the x-axis labels
labels = [col.replace("genre.", "") for col in top_genres.index]

# Create bar chart
# plt.figure(figsize=(20, 10))
plt.bar(labels, top_genres.values, color='purple')

# Add labels and title
plt.xlabel("Genre")
plt.xticks(rotation=45, ha='right')
plt.ylabel("Number of Titles")
plt.title("Top 10 Genres by Number of Titles")

# Display chart
plt.show()
```
:::

```{python}
# | echo: false
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(os.path.abspath("__file__")), '../Modeling')))
import ModelHelpers
import joblib
import sklearn
```

These visualizations revealed several intriguing patterns. We almost have an 80/20 split of movies and shows within our data. The number of released titles per year is exponential, and is only hindered in 2020, due to the COVID-19 pandemic limiting production. The few bar plots also demonstrate the ratings, major countries, and the types of media within the dataset. Now we have a much better picture of the data that we are working with and can keep this in mind for our future models.

#### Methods & Results

## Overview

In this section, we describe the machine learning models employed for predicting the popularity score of TV shows and movies in our project. We utilized several models, including beta regression, decision tree, K-Nearest Neighbors (KNN), and random forest. Each of these models offers unique characteristics and can capture different aspects of the data to make accurate predictions.

## Train-Test Split

To evaluate the performance of our machine learning models, we employed a train-test split approach. This process involves dividing our dataset, consisting of TV shows and movies, into two separate subsets: a training set and a testing set. The training set, which constitutes a majority of the data, was used to train our models to learn the underlying patterns and relationships between the features and the target variable, which in our case is the popularity score. The testing set, on the other hand, served as an unseen dataset to assess the models' generalization ability and determine their predictive performance on new, unseen instances. By randomly assigning the data points to the training and testing sets, we ensured that the evaluation process is unbiased and representative of real-world scenarios. After fitting each model with the training set, we are left with the root mean squared error (rMSE) value.

This metric gives us the weighted distance our model is from the correct metric. We found that the response variable `score` has a sample standard deviation of 21.7. This means that if we guessed the mean every time, we would be, on average, off by 21.7 points. So, our model should try to minimize the rMSE and get at most a rMSE of 21.7.

## Additional Information

For all of these models, we defined the set of predictor variables to be the same. These predictors included genre information, duration, release year, type, rating, director's average score, cast's average score, and country.

Also, to evaluate all four model's performance and validate its predictive ability, we employed a cross-validation strategy. Code was written so that the dataset was divided into five subsets or folds, with each fold serving as a testing set while the remaining four folds were used for training. The grid search was performed within this cross-validation framework, optimizing the model's hyperparameters based on the negative root mean squared error (RMSE) metric. By utilizing cross-validation, we obtained robust estimates of the model's performance and ensured the generalization of the results to unseen data.

# Modeling

::: panel-tabset
#### Beta Regression

## Beta Regression

Beta regression is a statistical model specifically designed for modeling continuous variables bounded between 0 and 1. In our case, we used beta regression to predict the popularity score, which ranges from 0 to 100. Beta regression takes into account the distributional characteristics of the data and can handle the inherent boundedness of the popularity score, allowing for accurate predictions. It uses an underlying logistic function to transform a linear regression.

![Sigmoid](Images/sigmoid.png)

We decided to fit a beta regression as opposed to a linear regression because we wanted the model to predict values only within the range \[0,100\]. A beta regression allows us to specify this range.

### Pipeline

Our pipeline was structured as follows:

```{python}
# | echo : false
# | warning: false
joblib.load("../Modeling/models/beta_regression.joblib")
```

First, we selected the predictors we wanted to use. Then, we one hot encoded the categorical variables. Simultaneously, we dealt with missing numerical values by encoding if a value was missing and filling it with a 0 if it was. We add our model as the last step in the pipeline.

### Cross Evaluation

The best cross-evaluation score for this model was a **root mean squared error** of **19.26**. We can loosely interpret this as saying our model is, on average, 19.26 IMDb rating points off from the actual score.

## Results

### Test rMSE: 19.71

The beta regression achieved a **test rMSE** of **19.71**. This is about a 2 point difference from the sample standard deviation of the scores (21.7), which means the beta regression predicts about 2 rating points more accurately than guessing the mean, on average.

#### KNN

## K-Nearest Neighbors

K-Nearest Neighbors is a non-parametric algorithm that makes predictions based on the similarity of a given data point to its k nearest neighbors in the feature space. In our case, KNN is used to predict the popularity score by finding the k most similar instances in the training data. KNN would be particularly suitable if similar shows or movies tend to have similar scores.

![KNN](Images/knn_visual.png)

### Pipeline

Our pipeline was structured as follows:

```{python}
# | echo : false
# | warning: false
joblib.load("../Modeling/models/knn.joblib")
```

This is the same pipeline as in the beta regression, with the model swapped out for KNN.

### Hyperparameter Tuning and Cross Evaluation

For KNN, we tuned two hyperparameters: `n_neighbors` and `weights`. `n_neighbors` controls the number of nearest neighbors to be used when making a prediction. `weights` controls how the points are averaged to make that prediction: uniformly or by distance. We found that the best value for `n_neighbors` was 17, and the best method for `weights` was by weighting by distance. The highest cross evaluation score for KNN was a **root mean squared error** of **20.48**.

## Results

### Test rMSE: 20.52

The K-Nearest Neighbors model achieved a **test rMSE** of **20.52**. This is about a 1 point difference from the sample standard deviation of the scores (21.7), which means the beta regression predicts about 1 rating point more accurately than guessing the mean, on average.


#### Decision Tree

## Decision Tree Regressor

Decision trees are intuitive models that create a flowchart-like structure of decisions and their potential consequences. Each internal node represents a decision based on a specific feature, and each leaf node represents the predicted outcome (popularity score). Decision trees are capable of handling both numerical and categorical features, and they offer interpretability by visualizing the decision-making process.

![Decision Tree](Images/dt.jpeg)

### Pipeline

Our pipeline is structured in the same way as our other models:

```{python}
# | echo : false
# | warning: false
joblib.load("../Modeling/models/decision_tree.joblib")
```

### Hyperparameter Tuning and Cross Evaluation
For the decision tree, we chose to tune two hyperparameters: `max_depth` and `min_samples_leaf`. Both of these hyperparameters can help with overfitting, which decision trees are notorious to do if left unchecked. The decision tree got a cross evaluation **root mean squared error** of **20.20**.

## Results

### Test rMSE: 19.84

The Decision Tree achieved a **test rMSE** of **19.84**. This is about a 2 point difference from the sample standard deviation of the scores (21.7), which means the beta regression predicts about 2 rating points more accurately than guessing the mean, on average.

#### Random Forest

## Random Forest Regressor

Random forest is model that combines multiple decision trees to make predictions. It averages across the individual tree predictions. Random forest can handle non-linear relationships, capture feature interactions, and effectively handle high-dimensional data. It provides robust predictions and reduces the risk of overfitting compared to individual decision trees.

![Random Forest](Images/A9E60566-EAB8-429C-A9ED-B867B2EC179A_1_201_a.jpeg)

### Pipeline

Again, we set up our pipeline as follows:

```{python}
# | echo : false
# | warning: false
joblib.load("../Modeling/models/random_forest.joblib")
```

### Hyperparameter Tuning and Cross Evaluation

Because a random forest contains many decision trees, you can tune both the hyperparameters of the forest and of the trees. We chose to again to tune `n_neighbors` and `weights`, and an additional hyperparameter `n_estimators`. This additional parameter controls the number of trees in the forest. Typically, more trees means a better model, and that is exactly what we found here, with `n_estimators` = 300. There seemed to be a drop off in the increase in performance around at 300 trees, so we didn't search for values of `n_estimators` farther than that. The cross evaluation score of the random forest was a **root mean squared error** of **18.94**.

## Results

### Test rMSE: 19.03

The Decision Tree achieved a **test rMSE** of **19.03**. This is almost a 3 point difference from the sample standard deviation of the scores (21.7), which means the beta regression predicts almost 3 rating points more accurately than guessing the mean, on average.

:::

#### App

Here, you can input parameters for a Movie or TV Show and see each our models' predictions for your show.

<iframe src="https://tv-popularity-model.streamlit.app//?embed=true" height="750" style="width:100%;border:none;">

</iframe>
:::
